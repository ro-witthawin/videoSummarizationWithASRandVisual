import os
import json
import torch
import librosa
import numpy as np
import soundfile as sf
from speechbrain.pretrained import EncoderClassifier
from pyannote.audio import Pipeline
import nemo.collections.asr as nemo_asr
from pydub import AudioSegment

# ============================
# CONFIG
# ============================
device = "cuda" if torch.cuda.is_available() else "cpu"
DIAR_MODEL = "pyannote/speaker-diarization@2.1"
HF_TOKEN = "hf_XXXXXXXXXXXXXXXXXXX"  # YOUR TOKEN
ASR_MODEL_NAME = "scb10x/typhoon-asr-realtime"
SIM_THRESHOLD = 0.75   # higher = stricter matching

AUDIO_DIR = "audioChunks"
OUTPUT_JSON = "merged_transcript.json"

# ============================
# LOAD MODELS
# ============================
print("Loading ASR model...")
asr_model = nemo_asr.models.ASRModel.from_pretrained(
    model_name=ASR_MODEL_NAME,
    map_location=device
)

print("Loading diarization model...")
diar_pipeline = Pipeline.from_pretrained(
    DIAR_MODEL,
    use_auth_token=HF_TOKEN
)

print("Loading speaker embedding model...")
spk_model = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": device}
)

# ============================
# UTIL FUNCTIONS
# ============================

def load_audio(path):
    """Loads audio and convert to wav 16k mono internally."""
    audio = AudioSegment.from_file(path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0
    return samples, 16000

def get_embedding(waveform, sr):
    """Extract speaker embedding vector."""
    print(waveform.shape)
    if waveform.ndim == 1:
        waveform = torch.tensor(waveform).unsqueeze(0)
    return spk_model.encode_batch(waveform.to(device)).detach().cpu().numpy()[0]

def cosine(a, b):
    return np.dot(a.flatten(), b.flatten()) / (np.linalg.norm(a) * np.linalg.norm(b))

def match_speaker(global_db, emb_vec):
    """Match embedding to global speakers DB."""
    if len(global_db) == 0:
        return None

    best_id, best_score = None, -1
    for spk_id, vecs in global_db.items():
        avg_vec = np.mean(vecs, axis=0)
        score = cosine(avg_vec, emb_vec)
        if score > best_score:
            best_id = spk_id
            best_score = score

    return best_id if best_score >= SIM_THRESHOLD else None


# ============================
# MAIN PIPELINE
# ============================
def process_chunks():
    audio_files = sorted([
        os.path.join(AUDIO_DIR, f)
        for f in os.listdir(AUDIO_DIR)
        if f.lower().endswith((".wav", ".mp3"))
    ])

    GLOBAL_SPK_DB = {}      # { "S1": [emb, emb, ...], ... }
    global_speaker_count = 1
    abs_time = 0
    full_output = []
    
    for file in audio_files:
        print(f"Processing: {file}")

        wav, sr = load_audio(file)

        # ---- 1. DIARIZATION ----
        diar = diar_pipeline({"waveform": torch.tensor(wav).unsqueeze(0), "sample_rate": sr})
        print(f"  Detected speakers: {diar.labels()}")
        segments = []
        for turn, _, spk in diar.itertracks(yield_label=True):
            try:
                seg_wav = wav[int(turn.start * sr): int(turn.end * sr)]
                emb = get_embedding(seg_wav, sr)

                # ---- 2. GLOBAL SPEAKER MATCHING ----
                match_id = match_speaker(GLOBAL_SPK_DB, emb)
                print(f"  Local speaker: {spk} --> Global speaker: {match_id}")

                if match_id is None:
                    match_id = f"S{global_speaker_count}"
                    GLOBAL_SPK_DB.setdefault(match_id, []).append(emb)
                    global_speaker_count += 1
                    print(f"    New global speaker created: {match_id}")
                else:
                    GLOBAL_SPK_DB[match_id].append(emb)
                    print(f"    Matched with existing global speaker: {match_id}")

                segments.append({
                    "local_spk": spk,
                    "global_spk": match_id,
                    "start": float(turn.start),
                    "end": float(turn.end),
                    "emb": emb
                })
                
                sf.write("tmp.wav", seg_wav, sr)
                texts = asr_model.transcribe("tmp.wav", timestamps=True)[0].timestamp["word"]
                for w in texts:
                    # print(f"start: {seg['start']}, {w['start']}, end: {seg['end']}, {w['end']}")
                    full_output.append({
                        "speaker": match_id,
                        "word": w["word"],
                        "start": float(turn.start) + abs_time + w["start"],
                        "end": float(turn.end) + abs_time + w["end"],
                        "audio": file
                    })
            except Exception as e:
                print("    Error processing segment:", e)
                continue

        abs_time += len(wav) / sr
        print("full_output :", full_output)

    return full_output


# ============================
# MERGE TO FINAL JSON
# ============================
def merge_to_json(results):
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("Saved:", OUTPUT_JSON)


# ============================
# RUN
# ============================
if __name__ == "__main__":
    result = process_chunks()
    merge_to_json(result)
    print("Done!")
