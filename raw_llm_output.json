{
  "summary_md": "# Executive Summary\nThis session is a deep-dive introduction to **Multimodal Models (มลติโมเดล)**, how they differ from unimodal LMs, and how they enable applications such as image captioning, VQA, and multimodal RAG. The speaker (S2) walks through key concepts, challenges, workflows, and a hands-on workshop involving **image → text → embedding → vector DB → retrieval** using MongoDB Atlas.\n\nThe discussion spans fundamentals (what is a multimodal model), practical constraints (fine-tune vs RAG), research models (VLMs), and step-by-step implementation.\n\n# Speaker Highlights\n- **S1**: Opening casual remark (small moment of humor).\n- **S2 (main speaker)**: Full technical lecture covering:\n  - Definition of multimodal vs unimodal\n  - Why multimodal RAG matters\n  - Role of image captioning + text embedding\n  - Using MongoDB Atlas for vector storage\n  - Practical trade-offs: RAG vs fine-tuning\n  - Demonstration of image → caption → embedding → vector search\n- **S3 / S4 / S5**: Small interjections, questions, and reactions.\n- **Other participants**: Occasional Q&A and humorous interactions.\n\n# Timeline Summary (with image placeholders)\n\n### **0–5s — Opening banter**\nS1 comments about the display size. S2 begins self-introduction.\n\n![Image at 1.62s](frames/frame_1.62.jpg)\n\n### **22–40s — Topic introduction: What is LOM / LM / multimodal**\nS2 introduces the confusion between LM vs “LOM” (misheard terms), sets stage for shift toward multimodal.\n\n![Image at 24.43s](frames/frame_24.43.jpg)\n\n### **60–120s — Defining multimodal models**\nExplains how multimodal models accept inputs beyond text: images, audio, video. Introduces concept of generative capabilities.\n\n![Image at 97.98s](frames/frame_97.98.jpg)\n\n### **196–260s — Image + text as multimodal input**\nS2 explains image-as-input, text output, and why this qualifies as multimodal.\n\n![Image at 217.56s](frames/frame_217.56.jpg)\n\n### **318–350s — Encoding / Decoding pipeline**\nOverview of encoder→LM→decoder workflow and the need for paired datasets.\n\n![Image at 336.99s](frames/frame_336.99.jpg)\n\n### **369–395s — State of multimodal models today**\nMentions ChatGPT, Gemini, and others as close-to-ideal multimodal but not truly universal yet.\n\n![Image at 381.15s](frames/frame_381.15.jpg)\n\n### **418–454s — What we will learn today**\nWorkshop goal: focus only on **image + text**, the simplest multimodal pair.\n\n![Image at 431.76s](frames/frame_431.76.jpg)\n\n### **472–580s — Image Captioning & Vision-Language Matching**\nDefines image captioning, why it is needed, and how to align visual/text domains.\n\n![Image at 515.35s](frames/frame_515.35.jpg)\n\n### **743–820s — Fine-tuning vs RAG**\nCritical practical section: why companies often choose RAG over fine-tuning (cost, speed, simplicity).\n\n![Image at 754.38s](frames/frame_754.38.jpg)\n\n### **950–1150s — Why not embed images directly?**\nImportant Q&A: Why caption-first → text embedding is better for text→image retrieval.\n\n![Image at 986.49s](frames/frame_986.49.jpg)\n\n### **1400–1500s — True multimodal RAG concept**\nExplains concept of multimodal RAG where image/video/audio also become searchable through embeddings.\n\n![Image at 1453.23s](frames/frame_1453.23.jpg)\n\n### **1700–1880s — Workshop Dataset & Model**\nShows dataset (Pokémon images + captions), explains model choices such as ViT + transformers.\n\n![Image at 1713.55s](frames/frame_1713.55.jpg)\n\n### **1950–2060s — Preparing data for MongoDB Atlas**\nConvert images to base64, captions to embeddings, insert them into DB.\n\n![Image at 2031.84s](frames/frame_2031.84.jpg)\n\n### **2220–2260s — Vector Search Introduction**\nS2 shows the vector search process and how embeddings (dimension 1024) are stored.\n\n![Image at 2242.16s](frames/frame_2242.16.jpg)\n\n\n# Final Summary\nThis session provides a comprehensive overview of multimodal AI from fundamentals to implementation. Participants learn:\n- How multimodal models understand inputs across modalities\n- Why image captioning is essential for text-to-image retrieval\n- How embeddings unify representations\n- How vector databases (MongoDB Atlas) enable multimodal RAG\n- Practical considerations for using pretrained models vs training from scratch\n\nThe workshop bridges theory + hands-on practice, grounding abstract multimodal ideas into concrete workflows.\n",
  "selected_timestamps": [
    1.62,
    24.43,
    97.98,
    217.56,
    336.99,
    381.15,
    431.76,
    515.35,
    754.38,
    986.49
  ]
}
